{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6785bc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title siren.py\n",
    "\n",
    "# Based on https://github.com/EmilienDupont/coin\n",
    "from math import sqrt\n",
    "\n",
    "import einops\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "import gc\n",
    "\n",
    "class Sine(nn.Module):\n",
    "    \"\"\"Sine activation with scaling.\n",
    "\n",
    "    Args:\n",
    "        w0 (float): Omega_0 parameter from SIREN paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0=1.0):\n",
    "        super().__init__()\n",
    "        self.w0 = w0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.w0 * x)\n",
    "\n",
    "\n",
    "class SirenLayer(nn.Module):\n",
    "    \"\"\"Implements a single SIREN layer.\n",
    "\n",
    "    Args:\n",
    "        dim_in (int): Dimension of input.\n",
    "        dim_out (int): Dimension of output.\n",
    "        w0 (float):\n",
    "        c (float): c value from SIREN paper used for weight initialization.\n",
    "        is_first (bool): Whether this is first layer of model.\n",
    "        is_last (bool): Whether this is last layer of model. If it is, no\n",
    "            activation is applied and 0.5 is added to the output. Since we\n",
    "            assume all training data lies in [0, 1], this allows for centering\n",
    "            the output of the model.\n",
    "        use_bias (bool): Whether to learn bias in linear layer.\n",
    "        activation (torch.nn.Module): Activation function. If None, defaults to\n",
    "            Sine activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_in,\n",
    "        dim_out,\n",
    "        w0=30.0,\n",
    "        c=6.0,\n",
    "        is_first=False,\n",
    "        is_last=False,\n",
    "        use_bias=True,\n",
    "        activation=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.is_first = is_first\n",
    "        self.is_last = is_last\n",
    "\n",
    "        self.linear = nn.Linear(dim_in, dim_out, bias=use_bias)\n",
    "\n",
    "        # Initialize layers following SIREN paper\n",
    "        w_std = (1 / dim_in) if self.is_first else (sqrt(c / dim_in) / w0)\n",
    "        nn.init.uniform_(self.linear.weight, -w_std, w_std)\n",
    "        if use_bias:\n",
    "            nn.init.uniform_(self.linear.bias, -w_std, w_std)\n",
    "\n",
    "        self.activation = Sine(w0) if activation is None else activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        if self.is_last:\n",
    "            # We assume target data is in [0, 1], so adding 0.5 allows us to learn\n",
    "            # zero-centered features\n",
    "            out += 0\n",
    "        else:\n",
    "            out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Siren(nn.Module):\n",
    "    \"\"\"SIREN model.\n",
    "\n",
    "    Args:\n",
    "        dim_in (int): Dimension of input.\n",
    "        dim_hidden (int): Dimension of hidden layers.\n",
    "        dim_out (int): Dimension of output.\n",
    "        num_layers (int): Number of layers.\n",
    "        w0 (float): Omega 0 from SIREN paper.\n",
    "        w0_initial (float): Omega 0 for first layer.\n",
    "        use_bias (bool): Whether to learn bias in linear layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_in,\n",
    "        dim_hidden,\n",
    "        dim_out,\n",
    "        num_layers,\n",
    "        w0=30.0,\n",
    "        w0_initial=30.0,\n",
    "        use_bias=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_out = dim_out\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        layers = []\n",
    "        for ind in range(num_layers - 1):\n",
    "            is_first = ind == 0\n",
    "            layer_w0 = w0_initial if is_first else w0\n",
    "            layer_dim_in = dim_in if is_first else dim_hidden\n",
    "\n",
    "            layers.append(\n",
    "                SirenLayer(\n",
    "                    dim_in=layer_dim_in,\n",
    "                    dim_out=dim_hidden,\n",
    "                    w0=layer_w0,\n",
    "                    use_bias=use_bias,\n",
    "                    is_first=is_first,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        self.last_layer = SirenLayer(\n",
    "            dim_in=dim_hidden, dim_out=dim_out, w0=w0, use_bias=use_bias, is_last=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of SIREN model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor of shape (*, dim_in), where * means any\n",
    "                number of dimensions.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (*, dim_out).\n",
    "        \"\"\"\n",
    "        x = self.net(x)\n",
    "        return self.last_layer(x)\n",
    "\n",
    "\n",
    "class ModulatedSiren(Siren):\n",
    "    \"\"\"Modulated SIREN model.\n",
    "\n",
    "    Args:\n",
    "        dim_in (int): Dimension of input.\n",
    "        dim_hidden (int): Dimension of hidden layers.\n",
    "        dim_out (int): Dimension of output.\n",
    "        num_layers (int): Number of layers.\n",
    "        w0 (float): Omega 0 from SIREN paper.\n",
    "        w0_initial (float): Omega 0 for first layer.\n",
    "        use_bias (bool): Whether to learn bias in linear layer.\n",
    "        modulate_scale (bool): Whether to modulate with scales.\n",
    "        modulate_shift (bool): Whether to modulate with shifts.\n",
    "        use_latent (bool): If true, use a latent vector which is mapped to\n",
    "            modulations, otherwise use modulations directly.\n",
    "        latent_dim (int): Dimension of latent vector.\n",
    "        modulation_net_dim_hidden (int): Number of hidden dimensions of\n",
    "            modulation network.\n",
    "        modulation_net_num_layers (int): Number of layers in modulation network.\n",
    "            If this is set to 1 will correspond to a linear layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_in,\n",
    "        dim_hidden,\n",
    "        dim_out,\n",
    "        num_layers,\n",
    "        w0=30.0,\n",
    "        w0_initial=30.0,\n",
    "        use_bias=True,\n",
    "        modulate_scale=False,\n",
    "        modulate_shift=True,\n",
    "        use_latent=True,\n",
    "        latent_dim=64,\n",
    "        modulation_net_dim_hidden=64,\n",
    "        modulation_net_num_layers=1,\n",
    "        mu=0,\n",
    "        sigma=1,\n",
    "        last_activation=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            dim_in,\n",
    "            dim_hidden,\n",
    "            dim_out,\n",
    "            num_layers,\n",
    "            w0,\n",
    "            w0_initial,\n",
    "            use_bias,\n",
    "        )\n",
    "        # Must modulate at least one of scale and shift\n",
    "        assert modulate_scale or modulate_shift\n",
    "\n",
    "        self.modulate_scale = modulate_scale\n",
    "        self.modulate_shift = modulate_shift\n",
    "        self.w0 = w0\n",
    "        self.w0_initial = w0_initial\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.last_activation = (\n",
    "            nn.Identity() if last_activation is None else last_activation\n",
    "        )\n",
    "\n",
    "        # We modulate features at every *hidden* layer of the base network and\n",
    "        # therefore have dim_hidden * (num_layers - 1) modulations, since the\n",
    "        # last layer is not modulated\n",
    "        num_modulations = dim_hidden * (num_layers - 1)\n",
    "        if self.modulate_scale and self.modulate_shift:\n",
    "            # If we modulate both scale and shift, we have twice the number of\n",
    "            # modulations at every layer and feature\n",
    "            num_modulations *= 2\n",
    "\n",
    "        if use_latent:\n",
    "            self.modulation_net = LatentToModulation(\n",
    "                latent_dim,\n",
    "                num_modulations,\n",
    "                modulation_net_dim_hidden,\n",
    "                modulation_net_num_layers,\n",
    "            )\n",
    "\n",
    "        self.num_modulations = num_modulations\n",
    "\n",
    "    def modulated_forward(self, x, latent):\n",
    "        \"\"\"Forward pass of modulated SIREN model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Shape (batch_size, *, dim_in), where * refers to\n",
    "                any spatial dimensions, e.g. (height, width), (height * width,)\n",
    "                or (depth, height, width) etc.\n",
    "            latent (torch.Tensor): Shape (batch_size, latent_dim). If\n",
    "                use_latent=False, then latent_dim = num_modulations.\n",
    "\n",
    "        Returns:\n",
    "            Output features of shape (batch_size, *, dim_out).\n",
    "        \"\"\"\n",
    "        # Extract batch_size and spatial dims of x, so we can reshape output\n",
    "        x_shape = x.shape[:-1]\n",
    "        # Flatten all spatial dimensions, i.e. shape\n",
    "        # (batch_size, *, dim_in) -> (batch_size, num_points, dim_in)\n",
    "\n",
    "        x = x.view(x.shape[0], -1, x.shape[-1])\n",
    "\n",
    "        # Shape (batch_size, num_modulations)\n",
    "        modulations = self.modulation_net(latent)\n",
    "\n",
    "        # Split modulations into shifts and scales and apply them to hidden\n",
    "        # features.\n",
    "        mid_idx = (\n",
    "            self.num_modulations // 2\n",
    "            if (self.modulate_scale and self.modulate_shift)\n",
    "            else 0\n",
    "        )\n",
    "        idx = 0\n",
    "        for module in self.net:\n",
    "            if self.modulate_scale:\n",
    "                # Shape (batch_size, 1, dim_hidden). Note that we add 1 so\n",
    "                # modulations remain zero centered\n",
    "                scale = modulations[:, idx: idx +\n",
    "                                    self.dim_hidden].unsqueeze(1) + 1.0\n",
    "            else:\n",
    "                scale = 1.0\n",
    "\n",
    "            if self.modulate_shift:\n",
    "                # Shape (batch_size, 1, dim_hidden)\n",
    "                shift = modulations[\n",
    "                    :, mid_idx + idx: mid_idx + idx + self.dim_hidden\n",
    "                ].unsqueeze(1)\n",
    "            else:\n",
    "                shift = 0.0\n",
    "\n",
    "            x = module.linear(x)\n",
    "            x = scale * x + shift  # Broadcast scale and shift across num_points\n",
    "            x = module.activation(x)  # (batch_size, num_points, dim_hidden)\n",
    "\n",
    "            idx = idx + self.dim_hidden\n",
    "\n",
    "        # Shape (batch_size, num_points, dim_out)\n",
    "        out = self.last_activation(self.last_layer(x))\n",
    "        out = out * self.sigma + self.mu\n",
    "        # Reshape (batch_size, num_points, dim_out) -> (batch_size, *, dim_out)\n",
    "        return out.view(*x_shape, out.shape[-1])\n",
    "\n",
    "# hypernetwork mapping latent code to modulation\n",
    "class LatentToModulation(nn.Module):\n",
    "    \"\"\"Maps a latent vector to a set of modulations.\n",
    "    Args:\n",
    "        latent_dim (int):\n",
    "        num_modulations (int):\n",
    "        dim_hidden (int):\n",
    "        num_layers (int):\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, latent_dim, num_modulations, dim_hidden, num_layers, activation=nn.SiLU\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_modulations = num_modulations\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.num_layers = num_layers\n",
    "        self.activation = activation\n",
    "\n",
    "        if num_layers == 1:\n",
    "            self.net = nn.Linear(latent_dim, num_modulations)\n",
    "        else:\n",
    "            layers = [nn.Linear(latent_dim, dim_hidden), self.activation()]\n",
    "            if num_layers > 2:\n",
    "                for i in range(num_layers - 2):\n",
    "                    layers += [nn.Linear(dim_hidden, dim_hidden),\n",
    "                               self.activation()]\n",
    "            layers += [nn.Linear(dim_hidden, num_modulations)]\n",
    "            self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, latent):\n",
    "        return self.net(latent)\n",
    "\n",
    "\n",
    "class Bias(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.bias = nn.Parameter(torch.zeros(size), requires_grad=True)\n",
    "        # Add latent_dim attribute for compatibility with LatentToModulation model\n",
    "        self.latent_dim = size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2db18234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title coral.losses\n",
    "\n",
    "import torch\n",
    "\n",
    "mse_fn = torch.nn.MSELoss()\n",
    "per_element_mse_fn = torch.nn.MSELoss(reduction=\"none\")\n",
    "per_element_bce_fn = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "\n",
    "def per_element_multi_scale_fn(\n",
    "    model_output,\n",
    "    gt,\n",
    "    loss_name=\"mse\",\n",
    "    last_element=False,\n",
    "):\n",
    "    if loss_name == \"mse\":\n",
    "        loss_fn = per_element_mse_fn\n",
    "    elif loss_name == \"bce\":\n",
    "        loss_fn = per_element_bce_fn\n",
    "\n",
    "    N = gt.shape[0]\n",
    "    gt = gt.reshape(N, -1)\n",
    "    loss = [loss_fn(out.reshape(N, -1), gt) for out in model_output]\n",
    "\n",
    "    loss = torch.stack(loss)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def batch_multi_scale_fn(model_output, gt, loss_name=\"mse\", use_resized=False):\n",
    "    # if use_resized:\n",
    "    #    loss = [(out - gt_img)**2 for out, gt_img in zip(model_output['model_out']['output'], gt['img'])]\n",
    "    # else:\n",
    "    per_element_multi_scale_mse = per_element_multi_scale_fn(\n",
    "        model_output, gt, loss_name=loss_name\n",
    "    )\n",
    "    # Shape (batch_size,)\n",
    "    return per_element_multi_scale_mse.view(gt.shape[0], -1).mean(dim=1)\n",
    "\n",
    "\n",
    "def per_element_nll_fn(x, y):\n",
    "    num_examples = x.size()[0]\n",
    "\n",
    "    negative_log_likelihood = -(y * torch.log(x) + (1 - y) * torch.log(1 - x))\n",
    "\n",
    "    return negative_log_likelihood\n",
    "\n",
    "\n",
    "def per_element_rel_mse_fn(x, y, reduction=True):\n",
    "    num_examples = x.size()[0]\n",
    "\n",
    "    diff_norms = torch.norm(\n",
    "        x.reshape(num_examples, -1) - y.reshape(num_examples, -1), 2, 1\n",
    "    )\n",
    "    y_norms = torch.norm(y.reshape(num_examples, -1), 2, 1)\n",
    "\n",
    "    return diff_norms / y_norms\n",
    "\n",
    "\n",
    "def batch_mse_rel_fn(x1, x2):\n",
    "    \"\"\"Computes MSE between two batches of signals while preserving the batch\n",
    "    dimension (per batch element MSE).\n",
    "    Args:\n",
    "        x1 (torch.Tensor): Shape (batch_size, *).\n",
    "        x2 (torch.Tensor): Shape (batch_size, *).\n",
    "    Returns:\n",
    "        MSE tensor of shape (batch_size,).\n",
    "    \"\"\"\n",
    "    # Shape (batch_size, *)\n",
    "    # per_element_mse = per_element_mse_fn(x1, x2)\n",
    "    per_element_mse = per_element_rel_mse_fn(x1, x2)\n",
    "    # Shape (batch_size,)\n",
    "    return per_element_mse.view(x1.shape[0], -1).mean(dim=1)\n",
    "\n",
    "\n",
    "def batch_mse_fn(x1, x2):\n",
    "    \"\"\"Computes MSE between two batches of signals while preserving the batch\n",
    "    dimension (per batch element MSE).\n",
    "    Args:\n",
    "        x1 (torch.Tensor): Shape (batch_size, *).\n",
    "        x2 (torch.Tensor): Shape (batch_size, *).\n",
    "    Returns:\n",
    "        MSE tensor of shape (batch_size,).\n",
    "    \"\"\"\n",
    "    # Shape (batch_size, *)\n",
    "    per_element_mse = per_element_mse_fn(x1, x2)\n",
    "    # Shape (batch_size,)\n",
    "    return per_element_mse.view(x1.shape[0], -1).mean(dim=1)\n",
    "\n",
    "\n",
    "def batch_nll_fn(x1, x2):\n",
    "    per_element_nll = per_element_nll_fn(x1, x2)\n",
    "    return per_element_nll.view(x1.shape[0], -1).mean(dim=1)\n",
    "\n",
    "\n",
    "def mse2psnr(mse):\n",
    "    \"\"\"Computes PSNR from MSE, assuming the MSE was calculated between signals\n",
    "    lying in [0, 1].\n",
    "    Args:\n",
    "        mse (torch.Tensor or float):\n",
    "    \"\"\"\n",
    "    return -10.0 * torch.log10(mse)\n",
    "\n",
    "\n",
    "def psnr_fn(x1, x2):\n",
    "    \"\"\"Computes PSNR between signals x1 and x2. Note that the values of x1 and\n",
    "    x2 are assumed to lie in [0, 1].\n",
    "    Args:\n",
    "        x1 (torch.Tensor): Shape (*).\n",
    "        x2 (torch.Tensor): Shape (*).\n",
    "    \"\"\"\n",
    "    return mse2psnr(mse_fn(x1, x2))\n",
    "\n",
    "\n",
    "# from fno\n",
    "\n",
    "\n",
    "# loss function with rel/abs Lp loss\n",
    "class LpLoss(object):\n",
    "    def __init__(self, d=2, p=2, size_average=True, reduction=True):\n",
    "        super(LpLoss, self).__init__()\n",
    "\n",
    "        # Dimension and Lp-norm type are postive\n",
    "        assert d > 0 and p > 0\n",
    "\n",
    "        self.d = d\n",
    "        self.p = p\n",
    "        self.reduction = reduction\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def abs(self, x, y):\n",
    "        num_examples = x.size()[0]\n",
    "\n",
    "        # Assume uniform mesh\n",
    "        h = 1.0 / (x.size()[1] - 1.0)\n",
    "\n",
    "        all_norms = (h ** (self.d / self.p)) * torch.norm(\n",
    "            x.view(num_examples, -1) - y.view(num_examples, -1), self.p, 1\n",
    "        )\n",
    "\n",
    "        if self.reduction:\n",
    "            if self.size_average:\n",
    "                return torch.mean(all_norms)\n",
    "            else:\n",
    "                return torch.sum(all_norms)\n",
    "\n",
    "        return all_norms\n",
    "\n",
    "    def rel(self, x, y):\n",
    "        num_examples = x.size()[0]\n",
    "\n",
    "        diff_norms = torch.norm(\n",
    "            x.reshape(num_examples, -1) - y.reshape(num_examples, -1), self.p, 1\n",
    "        )\n",
    "        y_norms = torch.norm(y.reshape(num_examples, -1), self.p, 1)\n",
    "\n",
    "        if self.reduction:\n",
    "            if self.size_average:\n",
    "                return torch.mean(diff_norms / y_norms)\n",
    "            else:\n",
    "                return torch.sum(diff_norms / y_norms)\n",
    "\n",
    "        return diff_norms / y_norms\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        return self.rel(x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8104f9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title metalearning.py\n",
    "\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "\n",
    "import einops\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as cp\n",
    "from torch import autograd\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "# adapted from https://github.com/EmilienDupont/coinpp/blob/main/coinpp/metalearning.py\n",
    "\n",
    "def inner_loop(\n",
    "    func_rep,\n",
    "    modulations,\n",
    "    coordinates,\n",
    "    features,\n",
    "    inner_steps,\n",
    "    inner_lr,\n",
    "    is_train=False,\n",
    "    gradient_checkpointing=False,\n",
    "    loss_type=\"mse\",\n",
    "):\n",
    "    \"\"\"Performs inner loop, i.e. fits modulations such that the function\n",
    "    representation can match the target features.\n",
    "\n",
    "    Args:\n",
    "        func_rep (models.ModulatedSiren):\n",
    "        modulations (torch.Tensor): Shape (batch_size, latent_dim). -> here the modulations ARE the latents\n",
    "        coordinates (torch.Tensor): Coordinates at which function representation\n",
    "            should be evaluated. Shape (batch_size, *, coordinate_dim).\n",
    "        features (torch.Tensor): Target features for model to match. Shape\n",
    "            (batch_size, *, feature_dim).\n",
    "        inner_steps (int): Number of inner loop steps to take.\n",
    "        inner_lr (float): Learning rate for inner loop.\n",
    "        is_train (bool):\n",
    "        gradient_checkpointing (bool): If True uses gradient checkpointing. This\n",
    "            can massively reduce memory consumption.\n",
    "    \"\"\"\n",
    "    fitted_modulations = modulations\n",
    "\n",
    "\n",
    "    for step in range(inner_steps):\n",
    "        if gradient_checkpointing:\n",
    "            fitted_modulations = cp.checkpoint(\n",
    "                inner_loop_step,\n",
    "                func_rep,\n",
    "                fitted_modulations,\n",
    "                coordinates,\n",
    "                features,\n",
    "                torch.as_tensor(inner_lr),\n",
    "                torch.as_tensor(is_train),\n",
    "                torch.as_tensor(gradient_checkpointing),\n",
    "                loss_type,\n",
    "            )\n",
    "        else:\n",
    "            fitted_modulations = inner_loop_step(\n",
    "                func_rep,\n",
    "                fitted_modulations,\n",
    "                coordinates,\n",
    "                features,\n",
    "                inner_lr,\n",
    "                is_train,\n",
    "                gradient_checkpointing,\n",
    "                loss_type,\n",
    "            )\n",
    "    return fitted_modulations\n",
    "\n",
    "\n",
    "def inner_loop_step(\n",
    "    func_rep,\n",
    "    modulations,\n",
    "    coordinates,\n",
    "    features,\n",
    "    inner_lr,\n",
    "    is_train=False,\n",
    "    gradient_checkpointing=False,\n",
    "    loss_type=\"mse\",\n",
    "):\n",
    "    \"\"\"Performs a single inner loop step.\"\"\"\n",
    "    detach = not torch.is_grad_enabled() and gradient_checkpointing\n",
    "    batch_size = len(features)\n",
    "    if loss_type == \"mse\":\n",
    "        element_loss_fn = per_element_mse_fn\n",
    "    elif loss_type == \"bce\":\n",
    "        element_loss_fn = per_element_nll_fn\n",
    "    elif \"multiscale\" in loss_type:\n",
    "        loss_name = loss_type.split(\"-\")[1]\n",
    "        element_loss_fn = partial(\n",
    "            per_element_multi_scale_fn,\n",
    "            loss_name=loss_name,\n",
    "            last_element=False,\n",
    "        )\n",
    "\n",
    "    N, C = features.shape[0], features.shape[-1]\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        # Note we multiply by batch size here to undo the averaging across batch\n",
    "        # elements from the MSE function. Indeed, each set of modulations is fit\n",
    "        # independently and the size of the gradient should not depend on how\n",
    "        # many elements are in the batch\n",
    "\n",
    "        # dimension mismatch occurs here\n",
    "        features_recon = func_rep.modulated_forward(coordinates, modulations)\n",
    "\n",
    "\n",
    "        loss = element_loss_fn(features_recon, features).mean() * batch_size\n",
    "\n",
    "        # If we are training, we should create graph since we will need this to\n",
    "        # compute second order gradients in the MAML outer loop\n",
    "        grad = torch.autograd.grad(\n",
    "            loss,\n",
    "            modulations,\n",
    "            create_graph=is_train and not detach,\n",
    "        )[0]\n",
    "        # if clip_grad_value is not None:\n",
    "        #    nn.utils.clip_grad_value_(grad, clip_grad_value)\n",
    "    # Perform single gradient descent step ON THE LATENTS\n",
    "    return modulations - inner_lr * grad\n",
    "\n",
    "\n",
    "def outer_step(\n",
    "    func_rep,\n",
    "    coordinates,\n",
    "    features,\n",
    "    inner_steps,\n",
    "    inner_lr,\n",
    "    is_train=False,\n",
    "    return_reconstructions=False,\n",
    "    gradient_checkpointing=False,\n",
    "    loss_type=\"mse\",\n",
    "    modulations=0,\n",
    "    use_rel_loss=False,\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        coordinates (torch.Tensor): Shape (batch_size, *, coordinate_dim). Note this\n",
    "            _must_ have a batch dimension.\n",
    "        features (torch.Tensor): Shape (batch_size, *, feature_dim). Note this _must_\n",
    "            have a batch dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    if loss_type == \"mse\":\n",
    "        loss_fn = batch_mse_fn\n",
    "    elif loss_type == \"bce\":\n",
    "        loss_fn = batch_nll_fn\n",
    "    elif \"multiscale\" in loss_type:\n",
    "        loss_name = loss_type.split(\"-\")[1]\n",
    "        loss_fn = partial(batch_multi_scale_fn, loss_name=loss_name)\n",
    "\n",
    "    func_rep.zero_grad()\n",
    "    batch_size = len(coordinates)\n",
    "    if isinstance(func_rep, DDP):\n",
    "        func_rep = func_rep.module\n",
    "\n",
    "    modulations = modulations.requires_grad_()\n",
    "\n",
    "    feat = features.clone()\n",
    "    coords = coordinates.clone()\n",
    "\n",
    "    # Run inner loop. HERE MODULATIONS IS LATENT Z\n",
    "    modulations = inner_loop(\n",
    "        func_rep,\n",
    "        modulations,\n",
    "        coords,\n",
    "        feat,\n",
    "        inner_steps,\n",
    "        inner_lr,\n",
    "        is_train,\n",
    "        gradient_checkpointing,\n",
    "        loss_type,\n",
    "    )\n",
    "\n",
    "\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        features_recon = func_rep.modulated_forward(coordinates, modulations)\n",
    "        per_example_loss = loss_fn(features_recon, features)  # features\n",
    "        loss = per_example_loss.mean()\n",
    "\n",
    "    outputs = {\n",
    "        \"loss\": loss,\n",
    "        \"psnr\": mse2psnr(per_example_loss).mean().item(),\n",
    "        \"modulations\": modulations,\n",
    "    }\n",
    "\n",
    "    if return_reconstructions:\n",
    "        outputs[\"reconstructions\"] = (\n",
    "            features_recon[-1] if \"multiscale\" in loss_type else features_recon\n",
    "        )\n",
    "\n",
    "    if use_rel_loss:\n",
    "        rel_loss = (\n",
    "            batch_mse_rel_fn(features_recon[-1], features).mean()\n",
    "            if \"multiscale\" in loss_type\n",
    "            else batch_mse_rel_fn(features_recon, features).mean()\n",
    "        )\n",
    "        outputs[\"rel_loss\"] = rel_loss\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dcc93ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title training logic\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def train_inr(\n",
    "    inr,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    latent_dim,\n",
    "    inner_steps,\n",
    "    inner_lr,\n",
    "    lr_code,\n",
    "    meta_lr_code,\n",
    "    weight_decay_code,\n",
    "    lr_inr,\n",
    "    gamma_step,\n",
    "    ntrain,\n",
    "    ntest,\n",
    "    epochs,\n",
    "    saved_checkpoint,\n",
    "    checkpoint,\n",
    "    RESULTS_DIR,\n",
    "    run_name,\n",
    "    device,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train an INR model using the meta-learning framework with inner and outer loops.\n",
    "\n",
    "    Args:\n",
    "        inr (nn.Module): The modulated SIREN model.\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        test_loader (DataLoader): DataLoader for testing data.\n",
    "        latent_dim (int): Dimension of latent codes.\n",
    "        inner_steps (int): Number of inner-loop steps.\n",
    "        inner_lr (float): Learning rate for inner-loop optimization.\n",
    "        lr_code (float): Initial learning rate for alpha.\n",
    "        meta_lr_code (float): Learning rate for meta-updates to alpha.\n",
    "        weight_decay_code (float): Weight decay for alpha optimization.\n",
    "        lr_inr (float): Learning rate for INR parameters.\n",
    "        gamma_step (float): Learning rate decay factor for scheduler.\n",
    "        ntrain (int): Number of training samples.\n",
    "        ntest (int): Number of testing samples.\n",
    "        epochs (int): Number of epochs.\n",
    "        saved_checkpoint (bool): Whether to load from a saved checkpoint.\n",
    "        checkpoint (dict): The saved checkpoint data.\n",
    "        RESULTS_DIR (str): Directory to save results.\n",
    "        run_name (str): Name for the saved model and logs.\n",
    "        device (torch.device): Device for training (e.g., 'cuda' or 'cpu').\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Initialize alpha parameter\n",
    "    alpha = nn.Parameter(torch.Tensor([lr_code]).to(device))\n",
    "\n",
    "    # Set optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        [\n",
    "            {\"params\": inr.parameters(), \"lr\": lr_inr},\n",
    "            {\"params\": alpha, \"lr\": meta_lr_code, \"weight_decay\": weight_decay_code},\n",
    "        ],\n",
    "        lr=lr_inr,\n",
    "        weight_decay=0,\n",
    "    )\n",
    "\n",
    "    # Load from checkpoint if available\n",
    "    if saved_checkpoint:\n",
    "        inr.load_state_dict(checkpoint['inr'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_inr'])\n",
    "        epoch_start = checkpoint['epoch']\n",
    "        alpha = checkpoint['alpha']\n",
    "        best_loss = checkpoint['loss']\n",
    "        cfg = checkpoint['cfg']\n",
    "        print(\"epoch_start, alpha, best_loss\", epoch_start, alpha.item(), best_loss)\n",
    "        print(\"cfg : \", cfg)\n",
    "    else:\n",
    "        epoch_start = 0\n",
    "        best_loss = np.inf\n",
    "\n",
    "    # Initialize scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode=\"min\",\n",
    "        factor=gamma_step,\n",
    "        patience=500,\n",
    "        threshold=0.01,\n",
    "        threshold_mode=\"rel\",\n",
    "        cooldown=0,\n",
    "        min_lr=1e-5,\n",
    "        eps=1e-08,\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    for step in range(epoch_start, epochs):\n",
    "\n",
    "        rel_train_mse = 0\n",
    "        rel_test_mse = 0\n",
    "        fit_train_mse = 0\n",
    "        fit_test_mse = 0\n",
    "        use_rel_loss = step % 10 == 0\n",
    "        step_show = step % 1 == 0\n",
    "        step_show_last = step == epochs - 1\n",
    "\n",
    "        for substep, (images, modulations, coords, idx) in enumerate(train_loader):\n",
    "            inr.train()\n",
    "            images = images.to(device)\n",
    "            modulations = modulations.to(device)\n",
    "            coords = coords.to(device)\n",
    "            n_samples = images.shape[0]\n",
    "\n",
    "\n",
    "            outputs = outer_step(\n",
    "                inr,\n",
    "                coords,\n",
    "                images,\n",
    "                inner_steps,\n",
    "                alpha,\n",
    "                is_train=True,\n",
    "                return_reconstructions=False,\n",
    "                gradient_checkpointing=False,\n",
    "                use_rel_loss=use_rel_loss,\n",
    "                loss_type=\"mse\",\n",
    "                modulations=torch.zeros_like(modulations),\n",
    "            )\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs[\"loss\"].backward(create_graph=False)\n",
    "            nn.utils.clip_grad_value_(inr.parameters(), clip_value=1.0)\n",
    "            optimizer.step()\n",
    "            loss = outputs[\"loss\"].cpu().detach()\n",
    "            fit_train_mse += loss.item() * n_samples\n",
    "\n",
    "            # MLP regression\n",
    "            if use_rel_loss:\n",
    "                rel_train_mse += outputs[\"rel_loss\"].item() * n_samples\n",
    "\n",
    "        train_loss = fit_train_mse / ntrain\n",
    "\n",
    "        scheduler.step(train_loss)\n",
    "\n",
    "        if use_rel_loss:\n",
    "            rel_train_loss = rel_train_mse / ntrain\n",
    "\n",
    "        # evaluating on test\n",
    "        if True in (step_show, step_show_last):\n",
    "            for images, modulations, coords, idx in test_loader:\n",
    "                inr.eval()\n",
    "                images = images.to(device)\n",
    "                modulations = modulations.to(device)\n",
    "                coords = coords.to(device)\n",
    "                n_samples = images.shape[0]\n",
    "\n",
    "                outputs = outer_step(\n",
    "                    inr,\n",
    "                    coords,\n",
    "                    images,\n",
    "                    inner_steps,\n",
    "                    alpha,\n",
    "                    is_train=False,\n",
    "                    return_reconstructions=False,\n",
    "                    gradient_checkpointing=False,\n",
    "                    use_rel_loss=use_rel_loss,\n",
    "                    loss_type=\"mse\",\n",
    "                    modulations=torch.zeros_like(modulations),\n",
    "                )\n",
    "\n",
    "                loss = outputs[\"loss\"]\n",
    "                fit_test_mse += loss.item() * n_samples\n",
    "\n",
    "                if use_rel_loss:\n",
    "                    rel_test_mse += outputs[\"rel_loss\"].item() * n_samples\n",
    "\n",
    "            test_loss = fit_test_mse / ntest\n",
    "\n",
    "            if use_rel_loss:\n",
    "                rel_test_loss = rel_test_mse / ntest\n",
    "\n",
    "            print(f'epoch = {step}; train_loss = {train_loss}; test_loss={test_loss}')\n",
    "\n",
    "        # Save the best model\n",
    "        if train_loss < best_loss:\n",
    "            best_loss = train_loss\n",
    "            print('save model')\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": step,\n",
    "                    \"inr\": inr,\n",
    "                    \"loss\": best_loss,\n",
    "                    \"alpha\": alpha,\n",
    "                },\n",
    "                f\"{RESULTS_DIR}/{run_name}.pt\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61c6e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define a synthetic dataset\n",
    "class SyntheticDataset(Dataset):\n",
    "    def __init__(self, num_samples, num_points, input_dim=2, output_dim=1, latent_dim=16):\n",
    "        self.num_samples = num_samples\n",
    "        self.num_points = num_points\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Generate random coordinates (x, y, ...)\n",
    "        self.coordinates = torch.rand(num_samples, num_points, input_dim)\n",
    "        # Generate corresponding outputs (e.g., a sine wave)\n",
    "        self.images = torch.sin(2 * torch.pi * self.coordinates.sum(dim=-1, keepdim=True))\n",
    "        # Initialize modulations with the correct dimensions\n",
    "        self.modulations = torch.zeros(num_samples, self.latent_dim)\n",
    "\n",
    "        print(self.coordinates.shape, self.images.shape, self.modulations.shape)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.modulations[idx], self.coordinates[idx], torch.tensor([idx])\n",
    "    \n",
    "    \n",
    "class FunctionDataset(Dataset):\n",
    "    def __init__(self, coordinates, features, latent_dim):\n",
    "        self.coordinates = coordinates\n",
    "        self.features = features\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        num_samples = self.coordinates.shape[0]\n",
    "        self.modulations = torch.zeros(num_samples, self.latent_dim)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.coordinates.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.modulations[idx], self.coordinates[idx], torch.tensor([idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bc9b4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now train an MLP from latent_a to latent_u\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class LatentCodeMLP(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dim=128, num_layers=3):\n",
    "        \"\"\"\n",
    "        A basic MLP to map one latent code to another.\n",
    "\n",
    "        Args:\n",
    "            latent_dim (int): Dimension of the input and output latent codes.\n",
    "            hidden_dim (int): Dimension of the hidden layers.\n",
    "            num_layers (int): Number of layers in the MLP (including input and output).\n",
    "        \"\"\"\n",
    "        super(LatentCodeMLP, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                layers.append(nn.Linear(latent_dim, hidden_dim))\n",
    "            elif i == num_layers - 1:\n",
    "                layers.append(nn.Linear(hidden_dim, latent_dim))\n",
    "            else:\n",
    "                layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            \n",
    "            if i != num_layers - 1:\n",
    "                layers.append(nn.SiLU())\n",
    "\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "def train_latent_code_mlp(model, dataset_a_train, dataset_u_train, dataset_a_test, dataset_u_test, latent_dim, epochs, lr, batch_size, device):\n",
    "    \"\"\"\n",
    "    Train the MLP to map latent codes from one dataset to another.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The MLP model.\n",
    "        dataset_a_train (torch.Tensor): Training latent codes from dataset `a` (shape: [num_train_samples, latent_dim]).\n",
    "        dataset_u_train (torch.Tensor): Training latent codes from dataset `u` (shape: [num_train_samples, latent_dim]).\n",
    "        dataset_a_test (torch.Tensor): Testing latent codes from dataset `a` (shape: [num_test_samples, latent_dim]).\n",
    "        dataset_u_test (torch.Tensor): Testing latent codes from dataset `u` (shape: [num_test_samples, latent_dim]).\n",
    "        latent_dim (int): Dimension of the latent codes.\n",
    "        epochs (int): Number of training epochs.\n",
    "        lr (float): Learning rate.\n",
    "        batch_size (int): Batch size for training.\n",
    "        device (torch.device): Device to use for training (CPU or GPU).\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: Trained MLP model.\n",
    "        list: Training losses per epoch.\n",
    "        list: Testing losses per epoch.\n",
    "    \"\"\"\n",
    "    # Create DataLoaders\n",
    "    train_dataset = torch.utils.data.TensorDataset(dataset_a_train, dataset_u_train)\n",
    "    test_dataset = torch.utils.data.TensorDataset(dataset_a_test, dataset_u_test)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for batch_a, batch_u in train_loader:\n",
    "            batch_a, batch_u = batch_a.to(device), batch_u.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(batch_a)\n",
    "            loss = criterion(predictions, batch_u)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * batch_a.size(0)\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Testing phase\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_a, batch_u in test_loader:\n",
    "                batch_a, batch_u = batch_a.to(device), batch_u.to(device)\n",
    "\n",
    "                predictions = model(batch_a)\n",
    "                loss = criterion(predictions, batch_u)\n",
    "\n",
    "                test_loss += loss.item() * batch_a.size(0)\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_losses.append(test_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}\")\n",
    "\n",
    "    return model, train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0685ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExtractEncoding(model_path, dataset):\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # EXTRACT THE CODES FROM INPUT INR\n",
    "\n",
    "    best_model = torch.load(model_path)\n",
    "\n",
    "    inr = best_model['inr']\n",
    "    alpha = best_model['alpha']\n",
    "    inner_steps = 3\n",
    "    \n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    encodings = []\n",
    "\n",
    "    for substep, (images, modulations, coords, idx) in enumerate(train_loader):\n",
    "        inr.train()\n",
    "        images = images.to(device)\n",
    "        modulations = modulations.to(device)\n",
    "        coords = coords.to(device)\n",
    "        n_samples = images.shape[0]\n",
    "\n",
    "        outputs = outer_step(\n",
    "          inr,\n",
    "          coords,\n",
    "          images,\n",
    "          inner_steps,\n",
    "          alpha,\n",
    "          is_train=True,\n",
    "          return_reconstructions=False,\n",
    "          gradient_checkpointing=False,\n",
    "          use_rel_loss=False,\n",
    "          loss_type=\"mse\",\n",
    "          modulations=torch.zeros_like(modulations),\n",
    "              )\n",
    "\n",
    "        encodings.append(outputs['modulations'])\n",
    "\n",
    "    # concatenate all the encodings\n",
    "    encodings_tensor = torch.concatenate(encodings, axis=0)\n",
    "\n",
    "    # for memory purposes\n",
    "    del best_model, inr\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return encodings_tensor.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4151f4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_generator(N, shape='box'):\n",
    "\n",
    "    # initialise\n",
    "    mask = np.zeros((N, N))\n",
    "\n",
    "    if shape=='box':\n",
    "        mask[1:-1, 1:-1] = 1\n",
    "\n",
    "    if shape=='torus1':\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if (i-N//2)**2 + (j-N//2)**2 < (N/2.2)**2:\n",
    "                    mask[i,j] = 1\n",
    "                if (i-N//2)**2 + (j-N//2)**2 < (N/6)**2:\n",
    "                    mask[i,j] = 0\n",
    "\n",
    "    if shape=='torus2':\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if (i-N//2)**2 + (j-N//2)**2 < (N/2.2)**2:\n",
    "                    mask[i,j] = 1\n",
    "                if (i-N//4)**2 + (j-N//2)**2 < (N/8)**2:\n",
    "                    mask[i,j] = 0\n",
    "                if (i-3*N//4)**2 + (j-N//2)**2 < (N/8)**2:\n",
    "                    mask[i,j] = 0\n",
    "\n",
    "    if shape=='triangle':\n",
    "        # Calculate the height of the triangle\n",
    "        triangle_height = N-5\n",
    "\n",
    "        # Center of the array\n",
    "        cx = N // 2\n",
    "\n",
    "        # Loop through the rows of the triangle\n",
    "        for y in range(triangle_height):\n",
    "            # Calculate the width of the triangle at the current row\n",
    "            half_width = y // 2\n",
    "\n",
    "            # Define the start and end indices for the row\n",
    "            start_x = cx - half_width\n",
    "            end_x = cx + half_width + 1\n",
    "\n",
    "            # Fill the row with 1s\n",
    "            mask[y, start_x:end_x] = 1\n",
    "\n",
    "    if shape=='bean':\n",
    "\n",
    "        num_points = 50\n",
    "        a, b, c = 1.2, 0,-0.2\n",
    "        circle_radius = 11.5\n",
    "\n",
    "        # Generate normalized x values for the quadratic curve\n",
    "        x_vals = np.linspace(-0.6, 0.6, num_points)\n",
    "        y_vals = a * x_vals**2 + b * x_vals + c\n",
    "\n",
    "        # Scale x and y values to pixel coordinates\n",
    "        x_pixel = ((x_vals + 1) * (N // 2)).astype(int)\n",
    "        y_pixel = ((1 - y_vals) * (N // 2)).astype(int)  # Invert y-axis for array indexing\n",
    "\n",
    "        # Draw circles along the curve\n",
    "        for x, y in zip(x_pixel, y_pixel):\n",
    "            # Create a grid of indices for the circle\n",
    "            y_grid, x_grid = np.ogrid[:N, :N]\n",
    "            distance = (x_grid - x)**2 + (y_grid - y)**2\n",
    "            mask[distance <= circle_radius**2] = 1\n",
    "\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcfdf15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2c08e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
