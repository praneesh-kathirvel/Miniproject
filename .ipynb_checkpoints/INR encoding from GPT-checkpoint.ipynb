{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71b1fc52",
   "metadata": {},
   "source": [
    "# CODE FROM GITHUB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61701c22",
   "metadata": {},
   "source": [
    "siren.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "955290de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://github.com/EmilienDupont/coin\n",
    "from math import sqrt\n",
    "\n",
    "import einops\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "#from coral.utils.interpolate import knn_interpolate_custom, rescale_coordinate\n",
    "\n",
    "class Sine(nn.Module):\n",
    "    \"\"\"Sine activation with scaling.\n",
    "\n",
    "    Args:\n",
    "        w0 (float): Omega_0 parameter from SIREN paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, w0=1.0):\n",
    "        super().__init__()\n",
    "        self.w0 = w0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.w0 * x)\n",
    "\n",
    "\n",
    "class SirenLayer(nn.Module):\n",
    "    \"\"\"Implements a single SIREN layer.\n",
    "\n",
    "    Args:\n",
    "        dim_in (int): Dimension of input.\n",
    "        dim_out (int): Dimension of output.\n",
    "        w0 (float):\n",
    "        c (float): c value from SIREN paper used for weight initialization.\n",
    "        is_first (bool): Whether this is first layer of model.\n",
    "        is_last (bool): Whether this is last layer of model. If it is, no\n",
    "            activation is applied and 0.5 is added to the output. Since we\n",
    "            assume all training data lies in [0, 1], this allows for centering\n",
    "            the output of the model.\n",
    "        use_bias (bool): Whether to learn bias in linear layer.\n",
    "        activation (torch.nn.Module): Activation function. If None, defaults to\n",
    "            Sine activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_in,\n",
    "        dim_out,\n",
    "        w0=30.0,\n",
    "        c=6.0,\n",
    "        is_first=False,\n",
    "        is_last=False,\n",
    "        use_bias=True,\n",
    "        activation=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.is_first = is_first\n",
    "        self.is_last = is_last\n",
    "\n",
    "        self.linear = nn.Linear(dim_in, dim_out, bias=use_bias)\n",
    "\n",
    "        # Initialize layers following SIREN paper\n",
    "        w_std = (1 / dim_in) if self.is_first else (sqrt(c / dim_in) / w0)\n",
    "        nn.init.uniform_(self.linear.weight, -w_std, w_std)\n",
    "        if use_bias:\n",
    "            nn.init.uniform_(self.linear.bias, -w_std, w_std)\n",
    "\n",
    "        self.activation = Sine(w0) if activation is None else activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        if self.is_last:\n",
    "            # We assume target data is in [0, 1], so adding 0.5 allows us to learn\n",
    "            # zero-centered features\n",
    "            out += 0\n",
    "        else:\n",
    "            out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Siren(nn.Module):\n",
    "    \"\"\"SIREN model.\n",
    "\n",
    "    Args:\n",
    "        dim_in (int): Dimension of input.\n",
    "        dim_hidden (int): Dimension of hidden layers.\n",
    "        dim_out (int): Dimension of output.\n",
    "        num_layers (int): Number of layers.\n",
    "        w0 (float): Omega 0 from SIREN paper.\n",
    "        w0_initial (float): Omega 0 for first layer.\n",
    "        use_bias (bool): Whether to learn bias in linear layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_in,\n",
    "        dim_hidden,\n",
    "        dim_out,\n",
    "        num_layers,\n",
    "        w0=30.0,\n",
    "        w0_initial=30.0,\n",
    "        use_bias=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_out = dim_out\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        layers = []\n",
    "        for ind in range(num_layers - 1):\n",
    "            is_first = ind == 0\n",
    "            layer_w0 = w0_initial if is_first else w0\n",
    "            layer_dim_in = dim_in if is_first else dim_hidden\n",
    "\n",
    "            layers.append(\n",
    "                SirenLayer(\n",
    "                    dim_in=layer_dim_in,\n",
    "                    dim_out=dim_hidden,\n",
    "                    w0=layer_w0,\n",
    "                    use_bias=use_bias,\n",
    "                    is_first=is_first,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        self.last_layer = SirenLayer(\n",
    "            dim_in=dim_hidden, dim_out=dim_out, w0=w0, use_bias=use_bias, is_last=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass of SIREN model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Tensor of shape (*, dim_in), where * means any\n",
    "                number of dimensions.\n",
    "\n",
    "        Returns:\n",
    "            Tensor of shape (*, dim_out).\n",
    "        \"\"\"\n",
    "        x = self.net(x)\n",
    "        return self.last_layer(x)\n",
    "\n",
    "\n",
    "class ModulatedSiren(Siren):\n",
    "    \"\"\"Modulated SIREN model.\n",
    "\n",
    "    Args:\n",
    "        dim_in (int): Dimension of input.\n",
    "        dim_hidden (int): Dimension of hidden layers.\n",
    "        dim_out (int): Dimension of output.\n",
    "        num_layers (int): Number of layers.\n",
    "        w0 (float): Omega 0 from SIREN paper.\n",
    "        w0_initial (float): Omega 0 for first layer.\n",
    "        use_bias (bool): Whether to learn bias in linear layer.\n",
    "        modulate_scale (bool): Whether to modulate with scales.\n",
    "        modulate_shift (bool): Whether to modulate with shifts.\n",
    "        use_latent (bool): If true, use a latent vector which is mapped to\n",
    "            modulations, otherwise use modulations directly.\n",
    "        latent_dim (int): Dimension of latent vector.\n",
    "        modulation_net_dim_hidden (int): Number of hidden dimensions of\n",
    "            modulation network.\n",
    "        modulation_net_num_layers (int): Number of layers in modulation network.\n",
    "            If this is set to 1 will correspond to a linear layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_in,\n",
    "        dim_hidden,\n",
    "        dim_out,\n",
    "        num_layers,\n",
    "        w0=30.0,\n",
    "        w0_initial=30.0,\n",
    "        use_bias=True,\n",
    "        modulate_scale=False,\n",
    "        modulate_shift=True,\n",
    "        use_latent=False,\n",
    "        latent_dim=64,\n",
    "        modulation_net_dim_hidden=64,\n",
    "        modulation_net_num_layers=1,\n",
    "        mu=0,\n",
    "        sigma=1,\n",
    "        last_activation=None,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            dim_in,\n",
    "            dim_hidden,\n",
    "            dim_out,\n",
    "            num_layers,\n",
    "            w0,\n",
    "            w0_initial,\n",
    "            use_bias,\n",
    "        )\n",
    "        # Must modulate at least one of scale and shift\n",
    "        assert modulate_scale or modulate_shift\n",
    "\n",
    "        self.modulate_scale = modulate_scale\n",
    "        self.modulate_shift = modulate_shift\n",
    "        self.w0 = w0\n",
    "        self.w0_initial = w0_initial\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.last_activation = (\n",
    "            nn.Identity() if last_activation is None else last_activation\n",
    "        )\n",
    "\n",
    "        # We modulate features at every *hidden* layer of the base network and\n",
    "        # therefore have dim_hidden * (num_layers - 1) modulations, since the\n",
    "        # last layer is not modulated\n",
    "        num_modulations = dim_hidden * (num_layers - 1)\n",
    "        if self.modulate_scale and self.modulate_shift:\n",
    "            # If we modulate both scale and shift, we have twice the number of\n",
    "            # modulations at every layer and feature\n",
    "            num_modulations *= 2\n",
    "\n",
    "        if use_latent:\n",
    "            self.modulation_net = LatentToModulation(\n",
    "                latent_dim,\n",
    "                num_modulations,\n",
    "                modulation_net_dim_hidden,\n",
    "                modulation_net_num_layers,\n",
    "            )\n",
    "        else:\n",
    "            self.modulation_net = Bias(num_modulations)\n",
    "\n",
    "        # Initialize scales to 1 and shifts to 0 (i.e. the identity)\n",
    "        if not use_latent:\n",
    "            if self.modulate_shift and self.modulate_scale:\n",
    "                self.modulation_net.bias.data = torch.cat(\n",
    "                    (\n",
    "                        torch.ones(num_modulations // 2),\n",
    "                        torch.zeros(num_modulations // 2),\n",
    "                    ),\n",
    "                    dim=0,\n",
    "                )\n",
    "            elif self.modulate_scale:\n",
    "                self.modulation_net.bias.data = torch.ones(num_modulations)\n",
    "            else:\n",
    "                self.modulation_net.bias.data = torch.zeros(num_modulations)\n",
    "\n",
    "        self.num_modulations = num_modulations\n",
    "\n",
    "    def modulated_forward(self, x, latent):\n",
    "        \"\"\"Forward pass of modulated SIREN model.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Shape (batch_size, *, dim_in), where * refers to\n",
    "                any spatial dimensions, e.g. (height, width), (height * width,)\n",
    "                or (depth, height, width) etc.\n",
    "            latent (torch.Tensor): Shape (batch_size, latent_dim). If\n",
    "                use_latent=False, then latent_dim = num_modulations.\n",
    "\n",
    "        Returns:\n",
    "            Output features of shape (batch_size, *, dim_out).\n",
    "        \"\"\"\n",
    "        # Extract batch_size and spatial dims of x, so we can reshape output\n",
    "        x_shape = x.shape[:-1]\n",
    "        # Flatten all spatial dimensions, i.e. shape\n",
    "        # (batch_size, *, dim_in) -> (batch_size, num_points, dim_in)\n",
    "\n",
    "        x = x.view(x.shape[0], -1, x.shape[-1])\n",
    "\n",
    "        # Shape (batch_size, num_modulations)\n",
    "        modulations = self.modulation_net(latent)\n",
    "\n",
    "        # Split modulations into shifts and scales and apply them to hidden\n",
    "        # features.\n",
    "        mid_idx = (\n",
    "            self.num_modulations // 2\n",
    "            if (self.modulate_scale and self.modulate_shift)\n",
    "            else 0\n",
    "        )\n",
    "        idx = 0\n",
    "        for module in self.net:\n",
    "            if self.modulate_scale:\n",
    "                # Shape (batch_size, 1, dim_hidden). Note that we add 1 so\n",
    "                # modulations remain zero centered\n",
    "                scale = modulations[:, idx: idx +\n",
    "                                    self.dim_hidden].unsqueeze(1) + 1.0\n",
    "            else:\n",
    "                scale = 1.0\n",
    "\n",
    "            if self.modulate_shift:\n",
    "                # Shape (batch_size, 1, dim_hidden)\n",
    "                shift = modulations[\n",
    "                    :, mid_idx + idx: mid_idx + idx + self.dim_hidden\n",
    "                ].unsqueeze(1)\n",
    "            else:\n",
    "                shift = 0.0\n",
    "\n",
    "            x = module.linear(x)\n",
    "            x = scale * x + shift  # Broadcast scale and shift across num_points\n",
    "            x = module.activation(x)  # (batch_size, num_points, dim_hidden)\n",
    "\n",
    "            idx = idx + self.dim_hidden\n",
    "\n",
    "        # Shape (batch_size, num_points, dim_out)\n",
    "        out = self.last_activation(self.last_layer(x))\n",
    "        out = out * self.sigma + self.mu\n",
    "        # Reshape (batch_size, num_points, dim_out) -> (batch_size, *, dim_out)\n",
    "        return out.view(*x_shape, out.shape[-1])\n",
    "\n",
    "\n",
    "class LatentToModulation(nn.Module):\n",
    "    \"\"\"Maps a latent vector to a set of modulations.\n",
    "    Args:\n",
    "        latent_dim (int):\n",
    "        num_modulations (int):\n",
    "        dim_hidden (int):\n",
    "        num_layers (int):\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, latent_dim, num_modulations, dim_hidden, num_layers, activation=nn.SiLU\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_modulations = num_modulations\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.num_layers = num_layers\n",
    "        self.activation = activation\n",
    "\n",
    "        if num_layers == 1:\n",
    "            self.net = nn.Linear(latent_dim, num_modulations)\n",
    "        else:\n",
    "            layers = [nn.Linear(latent_dim, dim_hidden), self.activation()]\n",
    "            if num_layers > 2:\n",
    "                for i in range(num_layers - 2):\n",
    "                    layers += [nn.Linear(dim_hidden, dim_hidden),\n",
    "                               self.activation()]\n",
    "            layers += [nn.Linear(dim_hidden, num_modulations)]\n",
    "            self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, latent):\n",
    "        return self.net(latent)\n",
    "\n",
    "\n",
    "class Bias(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super().__init__()\n",
    "        self.bias = nn.Parameter(torch.zeros(size), requires_grad=True)\n",
    "        # Add latent_dim attribute for compatibility with LatentToModulation model\n",
    "        self.latent_dim = size\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.bias\n",
    "\n",
    "\n",
    "class SSN(nn.Module):\n",
    "    \"\"\"Simple Sine model.\n",
    "\n",
    "    Args:\n",
    "        dim_in (int): Dimension of input.\n",
    "        dim_hidden (int): Dimension of hidden layers.\n",
    "        dim_out (int): Dimension of output.\n",
    "        num_layers (int): Number of layers.\n",
    "        w0 (float): Omega 0 from SIREN paper.\n",
    "        w0_initial (float): Omega 0 for first layer.\n",
    "        use_bias (bool): Whether to learn bias in linear layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_in,\n",
    "        dim_hidden,\n",
    "        dim_out,\n",
    "        num_layers,\n",
    "        w0=30.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_out = dim_out\n",
    "        self.num_layers = num_layers\n",
    "        self.w0 = w0\n",
    "\n",
    "        layers = [nn.Linear(dim_in, dim_hidden)]\n",
    "        for j in range(self.num_layers-1):\n",
    "            layers.append(nn.Linear(dim_hidden, dim_hidden))\n",
    "        layers.append(nn.Linear(dim_hidden, dim_out))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "        self.init_weights()\n",
    "                          \n",
    "    def init_weights(self):\n",
    "        for j, layer in enumerate(self.layers):\n",
    "            if j == 0:\n",
    "                nn.init.normal(layer.weight, 0,  np.sqrt(2) / np.sqrt(layer.weight.shape[1]))\n",
    "                #nn.init.uniform(layer.weight, -self.w0 / layer.weight.shape[1], self.w0 / layer.weight.shape[1])\n",
    "            else:\n",
    "                nn.init.normal(layer.weight, 0, np.sqrt(2) / np.sqrt(layer.weight.shape[1]))\n",
    "            print(layer.weight.std())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for j, layer in enumerate(self.layers[:-1]):\n",
    "            if j == 0:\n",
    "                x = torch.sin(self.w0*layer(x))\n",
    "            else:\n",
    "                x = torch.sin(layer(x))\n",
    "        out = self.layers[-1](x)\n",
    "        return out \n",
    "\n",
    "\n",
    "class ModulatedSSN(nn.Module):\n",
    "    \"\"\"Modulated Sine model.\n",
    "\n",
    "    Args:\n",
    "        dim_in (int): Dimension of input.\n",
    "        dim_hidden (int): Dimension of hidden layers.\n",
    "        dim_out (int): Dimension of output.\n",
    "        num_layers (int): Number of layers.\n",
    "        w0 (float): Omega 0 from SIREN paper.\n",
    "        w0_initial (float): Omega 0 for first layer.\n",
    "        use_bias (bool): Whether to learn bias in linear layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_in,\n",
    "        dim_hidden,\n",
    "        dim_out,\n",
    "        num_layers,\n",
    "        w0=30.0,\n",
    "        latent_dim=64,\n",
    "        modulation_net_dim_hidden=64,\n",
    "        modulation_net_num_layers=1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_hidden = dim_hidden\n",
    "        self.dim_out = dim_out\n",
    "        self.num_layers = num_layers\n",
    "        self.w0 = w0\n",
    "        self.latent_dim = latent_dim    \n",
    "\n",
    "        layers = [nn.Linear(dim_in, dim_hidden)]\n",
    "        for j in range(self.num_layers-1):\n",
    "            layers.append(nn.Linear(dim_hidden, dim_hidden))\n",
    "        layers.append(nn.Linear(dim_hidden, dim_out))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.num_modulations = dim_hidden * len(self.layers[:-1])    \n",
    "\n",
    "        self.modulation_net = LatentToModulation(\n",
    "                latent_dim,\n",
    "                self.num_modulations,\n",
    "                modulation_net_dim_hidden,\n",
    "                modulation_net_num_layers,\n",
    "            )\n",
    "        self.init_weights()\n",
    "                          \n",
    "    def init_weights(self):\n",
    "        for j, layer in enumerate(self.layers):\n",
    "            if j == 0:\n",
    "                nn.init.normal(layer.weight, 0,  np.sqrt(2) / np.sqrt(layer.weight.shape[1]))\n",
    "                #nn.init.uniform(layer.weight, -self.w0 / layer.weight.shape[1], self.w0 / layer.weight.shape[1])\n",
    "            else:\n",
    "                nn.init.normal(layer.weight, 0, np.sqrt(2) / np.sqrt(layer.weight.shape[1]))\n",
    "            print(layer.weight.std())\n",
    "        \n",
    "    def modulated_forward(self, x, z):\n",
    "        x_shape = x.shape[:-1]\n",
    "        x = x.view(x.shape[0], -1, x.shape[-1])\n",
    "        modulations = self.modulation_net(z)\n",
    "        modulations = modulations.reshape(-1, self.latent_dim, len(self.layers[:-1])).unsqueeze(1)\n",
    "\n",
    "        for j, layer in enumerate(self.layers[:-1]):\n",
    "            if j == 0:\n",
    "                x = torch.sin(self.w0*layer(x) + modulations[..., j])\n",
    "            else:\n",
    "                x = torch.sin(layer(x) + modulations[..., j])\n",
    "        out = self.layers[-1](x)\n",
    "\n",
    "        return out.view(*x_shape, out.shape[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1b54f3",
   "metadata": {},
   "source": [
    "coral.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0435bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "mse_fn = torch.nn.MSELoss()\n",
    "per_element_mse_fn = torch.nn.MSELoss(reduction=\"none\")\n",
    "per_element_bce_fn = torch.nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "\n",
    "def per_element_multi_scale_fn(\n",
    "    model_output,\n",
    "    gt,\n",
    "    loss_name=\"mse\",\n",
    "    last_element=False,\n",
    "):\n",
    "    if loss_name == \"mse\":\n",
    "        loss_fn = per_element_mse_fn\n",
    "    elif loss_name == \"bce\":\n",
    "        loss_fn = per_element_bce_fn\n",
    "\n",
    "    N = gt.shape[0]\n",
    "    gt = gt.reshape(N, -1)\n",
    "    loss = [loss_fn(out.reshape(N, -1), gt) for out in model_output]\n",
    "\n",
    "    loss = torch.stack(loss)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def batch_multi_scale_fn(model_output, gt, loss_name=\"mse\", use_resized=False):\n",
    "    # if use_resized:\n",
    "    #    loss = [(out - gt_img)**2 for out, gt_img in zip(model_output['model_out']['output'], gt['img'])]\n",
    "    # else:\n",
    "    per_element_multi_scale_mse = per_element_multi_scale_fn(\n",
    "        model_output, gt, loss_name=loss_name\n",
    "    )\n",
    "    # Shape (batch_size,)\n",
    "    return per_element_multi_scale_mse.view(gt.shape[0], -1).mean(dim=1)\n",
    "\n",
    "\n",
    "def per_element_nll_fn(x, y):\n",
    "    num_examples = x.size()[0]\n",
    "\n",
    "    negative_log_likelihood = -(y * torch.log(x) + (1 - y) * torch.log(1 - x))\n",
    "\n",
    "    return negative_log_likelihood\n",
    "\n",
    "\n",
    "def per_element_rel_mse_fn(x, y, reduction=True):\n",
    "    num_examples = x.size()[0]\n",
    "\n",
    "    diff_norms = torch.norm(\n",
    "        x.reshape(num_examples, -1) - y.reshape(num_examples, -1), 2, 1\n",
    "    )\n",
    "    y_norms = torch.norm(y.reshape(num_examples, -1), 2, 1)\n",
    "\n",
    "    return diff_norms / y_norms\n",
    "\n",
    "\n",
    "def batch_mse_rel_fn(x1, x2):\n",
    "    \"\"\"Computes MSE between two batches of signals while preserving the batch\n",
    "    dimension (per batch element MSE).\n",
    "    Args:\n",
    "        x1 (torch.Tensor): Shape (batch_size, *).\n",
    "        x2 (torch.Tensor): Shape (batch_size, *).\n",
    "    Returns:\n",
    "        MSE tensor of shape (batch_size,).\n",
    "    \"\"\"\n",
    "    # Shape (batch_size, *)\n",
    "    # per_element_mse = per_element_mse_fn(x1, x2)\n",
    "    per_element_mse = per_element_rel_mse_fn(x1, x2)\n",
    "    # Shape (batch_size,)\n",
    "    return per_element_mse.view(x1.shape[0], -1).mean(dim=1)\n",
    "\n",
    "\n",
    "def batch_mse_fn(x1, x2):\n",
    "    \"\"\"Computes MSE between two batches of signals while preserving the batch\n",
    "    dimension (per batch element MSE).\n",
    "    Args:\n",
    "        x1 (torch.Tensor): Shape (batch_size, *).\n",
    "        x2 (torch.Tensor): Shape (batch_size, *).\n",
    "    Returns:\n",
    "        MSE tensor of shape (batch_size,).\n",
    "    \"\"\"\n",
    "    # Shape (batch_size, *)\n",
    "    per_element_mse = per_element_mse_fn(x1, x2)\n",
    "    # Shape (batch_size,)\n",
    "    return per_element_mse.view(x1.shape[0], -1).mean(dim=1)\n",
    "\n",
    "\n",
    "def batch_nll_fn(x1, x2):\n",
    "    per_element_nll = per_element_nll_fn(x1, x2)\n",
    "    return per_element_nll.view(x1.shape[0], -1).mean(dim=1)\n",
    "\n",
    "\n",
    "def mse2psnr(mse):\n",
    "    \"\"\"Computes PSNR from MSE, assuming the MSE was calculated between signals\n",
    "    lying in [0, 1].\n",
    "    Args:\n",
    "        mse (torch.Tensor or float):\n",
    "    \"\"\"\n",
    "    return -10.0 * torch.log10(mse)\n",
    "\n",
    "\n",
    "def psnr_fn(x1, x2):\n",
    "    \"\"\"Computes PSNR between signals x1 and x2. Note that the values of x1 and\n",
    "    x2 are assumed to lie in [0, 1].\n",
    "    Args:\n",
    "        x1 (torch.Tensor): Shape (*).\n",
    "        x2 (torch.Tensor): Shape (*).\n",
    "    \"\"\"\n",
    "    return mse2psnr(mse_fn(x1, x2))\n",
    "\n",
    "\n",
    "# from fno\n",
    "\n",
    "\n",
    "# loss function with rel/abs Lp loss\n",
    "class LpLoss(object):\n",
    "    def __init__(self, d=2, p=2, size_average=True, reduction=True):\n",
    "        super(LpLoss, self).__init__()\n",
    "\n",
    "        # Dimension and Lp-norm type are postive\n",
    "        assert d > 0 and p > 0\n",
    "\n",
    "        self.d = d\n",
    "        self.p = p\n",
    "        self.reduction = reduction\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def abs(self, x, y):\n",
    "        num_examples = x.size()[0]\n",
    "\n",
    "        # Assume uniform mesh\n",
    "        h = 1.0 / (x.size()[1] - 1.0)\n",
    "\n",
    "        all_norms = (h ** (self.d / self.p)) * torch.norm(\n",
    "            x.view(num_examples, -1) - y.view(num_examples, -1), self.p, 1\n",
    "        )\n",
    "\n",
    "        if self.reduction:\n",
    "            if self.size_average:\n",
    "                return torch.mean(all_norms)\n",
    "            else:\n",
    "                return torch.sum(all_norms)\n",
    "\n",
    "        return all_norms\n",
    "\n",
    "    def rel(self, x, y):\n",
    "        num_examples = x.size()[0]\n",
    "\n",
    "        diff_norms = torch.norm(\n",
    "            x.reshape(num_examples, -1) - y.reshape(num_examples, -1), self.p, 1\n",
    "        )\n",
    "        y_norms = torch.norm(y.reshape(num_examples, -1), self.p, 1)\n",
    "\n",
    "        if self.reduction:\n",
    "            if self.size_average:\n",
    "                return torch.mean(diff_norms / y_norms)\n",
    "            else:\n",
    "                return torch.sum(diff_norms / y_norms)\n",
    "\n",
    "        return diff_norms / y_norms\n",
    "\n",
    "    def __call__(self, x, y):\n",
    "        return self.rel(x, y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dadd2aea",
   "metadata": {},
   "source": [
    "metalearning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faba3a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "\n",
    "import einops\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.checkpoint as cp\n",
    "from torch import autograd\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "# adapted from https://github.com/EmilienDupont/coinpp/blob/main/coinpp/metalearning.py\n",
    "\n",
    "def inner_loop(\n",
    "    func_rep,\n",
    "    modulations,\n",
    "    coordinates,\n",
    "    features,\n",
    "    inner_steps,\n",
    "    inner_lr,\n",
    "    is_train=False,\n",
    "    gradient_checkpointing=False,\n",
    "    loss_type=\"mse\",\n",
    "):\n",
    "    \"\"\"Performs inner loop, i.e. fits modulations such that the function\n",
    "    representation can match the target features.\n",
    "\n",
    "    Args:\n",
    "        func_rep (models.ModulatedSiren):\n",
    "        modulations (torch.Tensor): Shape (batch_size, latent_dim).\n",
    "        coordinates (torch.Tensor): Coordinates at which function representation\n",
    "            should be evaluated. Shape (batch_size, *, coordinate_dim).\n",
    "        features (torch.Tensor): Target features for model to match. Shape\n",
    "            (batch_size, *, feature_dim).\n",
    "        inner_steps (int): Number of inner loop steps to take.\n",
    "        inner_lr (float): Learning rate for inner loop.\n",
    "        is_train (bool):\n",
    "        gradient_checkpointing (bool): If True uses gradient checkpointing. This\n",
    "            can massively reduce memory consumption.\n",
    "    \"\"\"\n",
    "    fitted_modulations = modulations\n",
    "    \n",
    "    \n",
    "    for step in range(inner_steps):\n",
    "        if gradient_checkpointing:\n",
    "            fitted_modulations = cp.checkpoint(\n",
    "                inner_loop_step,\n",
    "                func_rep,\n",
    "                fitted_modulations,\n",
    "                coordinates,\n",
    "                features,\n",
    "                torch.as_tensor(inner_lr),\n",
    "                torch.as_tensor(is_train),\n",
    "                torch.as_tensor(gradient_checkpointing),\n",
    "                loss_type,\n",
    "            )\n",
    "        else:\n",
    "            fitted_modulations = inner_loop_step(\n",
    "                func_rep,\n",
    "                fitted_modulations,\n",
    "                coordinates,\n",
    "                features,\n",
    "                inner_lr,\n",
    "                is_train,\n",
    "                gradient_checkpointing,\n",
    "                loss_type,\n",
    "            )\n",
    "    return fitted_modulations\n",
    "\n",
    "\n",
    "def inner_loop_step(\n",
    "    func_rep,\n",
    "    modulations,\n",
    "    coordinates,\n",
    "    features,\n",
    "    inner_lr,\n",
    "    is_train=False,\n",
    "    gradient_checkpointing=False,\n",
    "    loss_type=\"mse\",\n",
    "):\n",
    "    \"\"\"Performs a single inner loop step.\"\"\"\n",
    "    detach = not torch.is_grad_enabled() and gradient_checkpointing\n",
    "    batch_size = len(features)\n",
    "    if loss_type == \"mse\":\n",
    "        element_loss_fn = per_element_mse_fn\n",
    "    elif loss_type == \"bce\":\n",
    "        element_loss_fn = per_element_nll_fn\n",
    "    elif \"multiscale\" in loss_type:\n",
    "        loss_name = loss_type.split(\"-\")[1]\n",
    "        element_loss_fn = partial(\n",
    "            per_element_multi_scale_fn,\n",
    "            loss_name=loss_name,\n",
    "            last_element=False,\n",
    "        )\n",
    "\n",
    "    N, C = features.shape[0], features.shape[-1]\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        # Note we multiply by batch size here to undo the averaging across batch\n",
    "        # elements from the MSE function. Indeed, each set of modulations is fit\n",
    "        # independently and the size of the gradient should not depend on how\n",
    "        # many elements are in the batch\n",
    "        features_recon = func_rep.modulated_forward(coordinates, modulations)\n",
    "\n",
    "        loss = element_loss_fn(features_recon, features).mean() * batch_size\n",
    "\n",
    "        # If we are training, we should create graph since we will need this to\n",
    "        # compute second order gradients in the MAML outer loop\n",
    "        grad = torch.autograd.grad(\n",
    "            loss,\n",
    "            modulations,\n",
    "            create_graph=is_train and not detach,\n",
    "        )[0]\n",
    "        # if clip_grad_value is not None:\n",
    "        #    nn.utils.clip_grad_value_(grad, clip_grad_value)\n",
    "    # Perform single gradient descent step\n",
    "    return modulations - inner_lr * grad\n",
    "\n",
    "\n",
    "def outer_step(\n",
    "    func_rep,\n",
    "    coordinates,\n",
    "    features,\n",
    "    inner_steps,\n",
    "    inner_lr,\n",
    "    is_train=False,\n",
    "    return_reconstructions=False,\n",
    "    gradient_checkpointing=False,\n",
    "    loss_type=\"mse\",\n",
    "    modulations=0,\n",
    "    use_rel_loss=False,\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        coordinates (torch.Tensor): Shape (batch_size, *, coordinate_dim). Note this\n",
    "            _must_ have a batch dimension.\n",
    "        features (torch.Tensor): Shape (batch_size, *, feature_dim). Note this _must_\n",
    "            have a batch dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    if loss_type == \"mse\":\n",
    "        loss_fn = batch_mse_fn\n",
    "    elif loss_type == \"bce\":\n",
    "        loss_fn = batch_nll_fn\n",
    "    elif \"multiscale\" in loss_type:\n",
    "        loss_name = loss_type.split(\"-\")[1]\n",
    "        loss_fn = partial(batch_multi_scale_fn, loss_name=loss_name)\n",
    "\n",
    "    func_rep.zero_grad()\n",
    "    batch_size = len(coordinates)\n",
    "    if isinstance(func_rep, DDP):\n",
    "        func_rep = func_rep.module\n",
    "\n",
    "    modulations = modulations.requires_grad_()\n",
    "\n",
    "    feat = features.clone()\n",
    "    coords = coordinates.clone()\n",
    "\n",
    "    # Run inner loop\n",
    "    modulations = inner_loop(\n",
    "        func_rep,\n",
    "        modulations,\n",
    "        coords,\n",
    "        feat,\n",
    "        inner_steps,\n",
    "        inner_lr,\n",
    "        is_train,\n",
    "        gradient_checkpointing,\n",
    "        loss_type,\n",
    "    )\n",
    "\n",
    "    with torch.set_grad_enabled(is_train):\n",
    "        features_recon = func_rep.modulated_forward(coordinates, modulations)\n",
    "        per_example_loss = loss_fn(features_recon, features)  # features\n",
    "        loss = per_example_loss.mean()\n",
    "\n",
    "    outputs = {\n",
    "        \"loss\": loss,\n",
    "        \"psnr\": mse2psnr(per_example_loss).mean().item(),\n",
    "        \"modulations\": modulations,\n",
    "    }\n",
    "\n",
    "    if return_reconstructions:\n",
    "        outputs[\"reconstructions\"] = (\n",
    "            features_recon[-1] if \"multiscale\" in loss_type else features_recon\n",
    "        )\n",
    "\n",
    "    if use_rel_loss:\n",
    "        rel_loss = (\n",
    "            batch_mse_rel_fn(features_recon[-1], features).mean()\n",
    "            if \"multiscale\" in loss_type\n",
    "            else batch_mse_rel_fn(features_recon, features).mean()\n",
    "        )\n",
    "        outputs[\"rel_loss\"] = rel_loss\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d00ade",
   "metadata": {},
   "source": [
    "# GPT integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2407d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load in data\n",
    "\n",
    "with open('donut_data.pkl', 'rb') as file:\n",
    "    data_list = pickle.load(file)\n",
    "    \n",
    "with open('donut_data_coord_grid.pkl', 'rb') as file:\n",
    "    coord_grid = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c490bf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_data = len(data_list)\n",
    "# split data \n",
    "data_train, data_test, data_val = data_list[0:1000], data_list[1000:1100], data_list[1100:1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5abb4cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prane\\AppData\\Local\\Temp\\ipykernel_21972\\217502826.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:278.)\n",
      "  coord_tensor = torch.tensor(coord_list, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "coord_list = []\n",
    "\n",
    "for i in range(coord_grid.shape[0]):\n",
    "    for j in range(coord_grid.shape[1]):\n",
    "        coord_list.append(coord_grid[i, j])\n",
    "        \n",
    "coord_tensor = torch.tensor(coord_list, dtype=torch.float32)    \n",
    "coordinates = coord_tensor.unsqueeze(0).repeat(len(data_train), 1, 1)\n",
    "\n",
    "a_list = []\n",
    "u_list = []\n",
    "\n",
    "features = torch.zeros(len(data_train), coord_grid.shape[0]*coord_grid.shape[1], 1, dtype=torch.float32)\n",
    "\n",
    "k = 0\n",
    "for a, u in data_train:\n",
    "    \n",
    "    flat_a = []\n",
    "    flat_u = []\n",
    "    \n",
    "    for i in range(coord_grid.shape[0]):\n",
    "        for j in range(coord_grid.shape[1]):\n",
    "            flat_a.append(a[i, j])\n",
    "            flat_u.append(u[i, j])\n",
    "            \n",
    "    features[k] = torch.tensor(flat_a).unsqueeze(-1)\n",
    "    k += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c8c0ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.float32, torch.float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coordinates.dtype, features.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "963d27e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer Step 1/25, Batch 1, Loss: 37.6305\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 82\u001b[0m\n\u001b[0;32m     69\u001b[0m modulations \u001b[38;5;241m=\u001b[39m inner_loop(\n\u001b[0;32m     70\u001b[0m     func_rep\u001b[38;5;241m=\u001b[39mfunc_rep,\n\u001b[0;32m     71\u001b[0m     modulations\u001b[38;5;241m=\u001b[39mmodulations,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     78\u001b[0m     loss_type\u001b[38;5;241m=\u001b[39mloss_type,\n\u001b[0;32m     79\u001b[0m )\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Outer loop: Update shared model parameters\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outer_step(\n\u001b[0;32m     83\u001b[0m     func_rep\u001b[38;5;241m=\u001b[39mfunc_rep,\n\u001b[0;32m     84\u001b[0m     coordinates\u001b[38;5;241m=\u001b[39mbatch_coords,\n\u001b[0;32m     85\u001b[0m     features\u001b[38;5;241m=\u001b[39mbatch_features,\n\u001b[0;32m     86\u001b[0m     inner_steps\u001b[38;5;241m=\u001b[39minner_steps,\n\u001b[0;32m     87\u001b[0m     inner_lr\u001b[38;5;241m=\u001b[39minner_lr,\n\u001b[0;32m     88\u001b[0m     is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     89\u001b[0m     return_reconstructions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     90\u001b[0m     gradient_checkpointing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     91\u001b[0m     loss_type\u001b[38;5;241m=\u001b[39mloss_type,\n\u001b[0;32m     92\u001b[0m     modulations\u001b[38;5;241m=\u001b[39mmodulations,\n\u001b[0;32m     93\u001b[0m )\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Extract loss\u001b[39;00m\n\u001b[0;32m     96\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[1;32mIn[4], line 160\u001b[0m, in \u001b[0;36mouter_step\u001b[1;34m(func_rep, coordinates, features, inner_steps, inner_lr, is_train, return_reconstructions, gradient_checkpointing, loss_type, modulations, use_rel_loss)\u001b[0m\n\u001b[0;32m    157\u001b[0m coords \u001b[38;5;241m=\u001b[39m coordinates\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m    159\u001b[0m \u001b[38;5;66;03m# Run inner loop\u001b[39;00m\n\u001b[1;32m--> 160\u001b[0m modulations \u001b[38;5;241m=\u001b[39m inner_loop(\n\u001b[0;32m    161\u001b[0m     func_rep,\n\u001b[0;32m    162\u001b[0m     modulations,\n\u001b[0;32m    163\u001b[0m     coords,\n\u001b[0;32m    164\u001b[0m     feat,\n\u001b[0;32m    165\u001b[0m     inner_steps,\n\u001b[0;32m    166\u001b[0m     inner_lr,\n\u001b[0;32m    167\u001b[0m     is_train,\n\u001b[0;32m    168\u001b[0m     gradient_checkpointing,\n\u001b[0;32m    169\u001b[0m     loss_type,\n\u001b[0;32m    170\u001b[0m )\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(is_train):\n\u001b[0;32m    173\u001b[0m     features_recon \u001b[38;5;241m=\u001b[39m func_rep\u001b[38;5;241m.\u001b[39mmodulated_forward(coordinates, modulations)\n",
      "Cell \u001b[1;32mIn[4], line 57\u001b[0m, in \u001b[0;36minner_loop\u001b[1;34m(func_rep, modulations, coordinates, features, inner_steps, inner_lr, is_train, gradient_checkpointing, loss_type)\u001b[0m\n\u001b[0;32m     45\u001b[0m         fitted_modulations \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m     46\u001b[0m             inner_loop_step,\n\u001b[0;32m     47\u001b[0m             func_rep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m             loss_type,\n\u001b[0;32m     55\u001b[0m         )\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 57\u001b[0m         fitted_modulations \u001b[38;5;241m=\u001b[39m inner_loop_step(\n\u001b[0;32m     58\u001b[0m             func_rep,\n\u001b[0;32m     59\u001b[0m             fitted_modulations,\n\u001b[0;32m     60\u001b[0m             coordinates,\n\u001b[0;32m     61\u001b[0m             features,\n\u001b[0;32m     62\u001b[0m             inner_lr,\n\u001b[0;32m     63\u001b[0m             is_train,\n\u001b[0;32m     64\u001b[0m             gradient_checkpointing,\n\u001b[0;32m     65\u001b[0m             loss_type,\n\u001b[0;32m     66\u001b[0m         )\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fitted_modulations\n",
      "Cell \u001b[1;32mIn[4], line 108\u001b[0m, in \u001b[0;36minner_loop_step\u001b[1;34m(func_rep, modulations, coordinates, features, inner_lr, is_train, gradient_checkpointing, loss_type)\u001b[0m\n\u001b[0;32m    104\u001b[0m     loss \u001b[38;5;241m=\u001b[39m element_loss_fn(features_recon, features)\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m*\u001b[39m batch_size\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m# If we are training, we should create graph since we will need this to\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;66;03m# compute second order gradients in the MAML outer loop\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgrad(\n\u001b[0;32m    109\u001b[0m         loss,\n\u001b[0;32m    110\u001b[0m         modulations,\n\u001b[0;32m    111\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mis_train \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m detach,\n\u001b[0;32m    112\u001b[0m     )[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# if clip_grad_value is not None:\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;66;03m#    nn.utils.clip_grad_value_(grad, clip_grad_value)\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;66;03m# Perform single gradient descent step\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m modulations \u001b[38;5;241m-\u001b[39m inner_lr \u001b[38;5;241m*\u001b[39m grad\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:411\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    407\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[0;32m    408\u001b[0m         grad_outputs_\n\u001b[0;32m    409\u001b[0m     )\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 411\u001b[0m     result \u001b[38;5;241m=\u001b[39m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    412\u001b[0m         t_outputs,\n\u001b[0;32m    413\u001b[0m         grad_outputs_,\n\u001b[0;32m    414\u001b[0m         retain_graph,\n\u001b[0;32m    415\u001b[0m         create_graph,\n\u001b[0;32m    416\u001b[0m         inputs,\n\u001b[0;32m    417\u001b[0m         allow_unused,\n\u001b[0;32m    418\u001b[0m         accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    419\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[0;32m    421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    422\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[0;32m    423\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[0;32m    424\u001b[0m     ):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Example dataset class for handling (coordinates, features)\n",
    "class ScalarFieldDataset(Dataset):\n",
    "    def __init__(self, coordinates, features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            coordinates (torch.Tensor): Tensor of shape (num_samples, num_points, dim_in).\n",
    "            features (torch.Tensor): Tensor of shape (num_samples, num_points, dim_out).\n",
    "        \"\"\"\n",
    "        self.coordinates = coordinates\n",
    "        self.features = features\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.coordinates)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.coordinates[idx], self.features[idx]\n",
    "\n",
    "# Generate sample data (placeholders for example purposes)\n",
    "#num_samples = 1000  # Total number of samples\n",
    "#num_points = 100    # Number of points per sample\n",
    "dim_in = 2          # Input dimension\n",
    "dim_out = 1         # Output dimension\n",
    "\n",
    "#coordinates = torch.rand(num_samples, num_points, dim_in)  # Input coordinates\n",
    "#features = torch.rand(num_samples, num_points, dim_out)    # Ground truth outputs\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = ScalarFieldDataset(coordinates, features)\n",
    "batch_size = 30\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)  # Batch size of 30\n",
    "\n",
    "# Initialize model\n",
    "func_rep = ModulatedSiren(\n",
    "    dim_in=dim_in,\n",
    "    dim_hidden=64,\n",
    "    dim_out=dim_out,\n",
    "    num_layers=5,\n",
    "    modulate_scale=True,\n",
    "    modulate_shift=True,\n",
    "    use_latent=True,\n",
    "    latent_dim=64,\n",
    "    modulation_net_dim_hidden=64,\n",
    "    modulation_net_num_layers=2,\n",
    ")\n",
    "\n",
    "# Optimizer for model parameters (outer loop)\n",
    "optimizer = optim.Adam(func_rep.parameters(), lr=1e-3)\n",
    "\n",
    "# Hyperparameters\n",
    "inner_steps = 5\n",
    "inner_lr = 1e-2\n",
    "outer_steps = 25\n",
    "loss_type = \"mse\"\n",
    "\n",
    "# Encoding process\n",
    "for outer_step_no in range(outer_steps):\n",
    "    for batch_idx, (batch_coords, batch_features) in enumerate(dataloader):\n",
    "        batch_size = batch_coords.shape[0]\n",
    "\n",
    "        # Initialize latent codes (modulations, z_a) for the batch\n",
    "        modulations = torch.zeros(batch_size, func_rep.modulation_net.latent_dim, requires_grad=True)\n",
    "\n",
    "        # Inner loop: Optimize modulations\n",
    "        modulations = inner_loop(\n",
    "            func_rep=func_rep,\n",
    "            modulations=modulations,\n",
    "            coordinates=batch_coords,\n",
    "            features=batch_features,\n",
    "            inner_steps=inner_steps,\n",
    "            inner_lr=inner_lr,\n",
    "            is_train=True,\n",
    "            gradient_checkpointing=False,\n",
    "            loss_type=loss_type,\n",
    "        )\n",
    "\n",
    "        # Outer loop: Update shared model parameters\n",
    "        outputs = outer_step(\n",
    "            func_rep=func_rep,\n",
    "            coordinates=batch_coords,\n",
    "            features=batch_features,\n",
    "            inner_steps=inner_steps,\n",
    "            inner_lr=inner_lr,\n",
    "            is_train=True,\n",
    "            return_reconstructions=False,\n",
    "            gradient_checkpointing=False,\n",
    "            loss_type=loss_type,\n",
    "            modulations=modulations,\n",
    "        )\n",
    "\n",
    "        # Extract loss\n",
    "        loss = outputs[\"loss\"]\n",
    "\n",
    "        # Print progress for the batch\n",
    "        if batch_idx == 0:\n",
    "          print(f\"Outer Step {outer_step_no + 1}/{outer_steps}, Batch {batch_idx + 1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Final outputs\n",
    "print(\"Encoding process completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33c62bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modulations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a71c62b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
